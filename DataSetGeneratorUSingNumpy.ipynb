{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1563,
     "status": "ok",
     "timestamp": 1734180378611,
     "user": {
      "displayName": "shadabkhan pathan",
      "userId": "14649001743684245130"
     },
     "user_tz": -330
    },
    "id": "PklGH3UhkJzE",
    "outputId": "48147014-a65e-4547-9dd2-ff7e8f7c9d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to fraud_detection_dataset.csv\n",
      "Label\n",
      "0    8338\n",
      "1    1662\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_fraud_detection_dataset(n_samples=10000, seed=42, output_file=\"fraud_detection_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Generates a synthetic fraud detection dataset with realistic features and fraud labeling logic.\n",
    "\n",
    "    Parameters:\n",
    "        n_samples (int): Number of samples to generate.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        output_file (str): Path to save the dataset as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None: Saves the dataset to the specified CSV file.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate features\n",
    "    data = {\n",
    "        \"Transaction Amount\": np.random.exponential(scale=500, size=n_samples),\n",
    "        \"Transaction Frequency\": np.random.poisson(lam=3, size=n_samples),\n",
    "        \"Recipient Verification Status\": np.random.choice(\n",
    "            [\"verified\", \"recently_registered\", \"suspicious\"], n_samples, p=[0.6, 0.3, 0.1]\n",
    "        ),\n",
    "        \"Recipient Blacklist Status\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"Device Fingerprinting\": np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "        \"VPN or Proxy Usage\": np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "        \"Geo-Location Flags\": np.random.choice(\n",
    "            [\"normal\", \"high-risk\", \"unusual\"], n_samples, p=[0.7, 0.2, 0.1]\n",
    "        ),\n",
    "        \"Behavioral Biometrics\": np.random.normal(loc=0, scale=1, size=n_samples),\n",
    "        \"Time Since Last Transaction\": np.random.uniform(0, 30, n_samples),\n",
    "        \"Social Trust Score\": np.random.uniform(0, 100, n_samples),\n",
    "        \"Account Age\": np.random.uniform(0, 5, n_samples),\n",
    "        \"High-Risk Transaction Times\": np.random.choice([0, 1], n_samples, p=[0.75, 0.25]),\n",
    "        \"Past Fraudulent Behavior Flags\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"Location-Inconsistent Transactions\": np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "        \"Normalized Transaction Amount\": np.random.normal(loc=0.5, scale=0.2, size=n_samples),\n",
    "        \"Transaction Context Anomalies\": np.random.normal(loc=0, scale=1, size=n_samples),\n",
    "        \"Fraud Complaints Count\": np.random.poisson(lam=0.5, size=n_samples),\n",
    "        \"Merchant Category Mismatch\": np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "        \"User Daily Limit Exceeded\": np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "        \"Recent High-Value Transaction Flags\": np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Labeling based on feature importance\n",
    "    def label_transaction(row):\n",
    "        fraud_score = 0\n",
    "        # Feature weights\n",
    "        fraud_score += row[\"Recipient Blacklist Status\"] * 3\n",
    "        fraud_score += row[\"Device Fingerprinting\"] * 2\n",
    "        fraud_score += row[\"VPN or Proxy Usage\"] * 2\n",
    "        fraud_score += 1 if row[\"Geo-Location Flags\"] == \"high-risk\" else 0\n",
    "        fraud_score += row[\"Past Fraudulent Behavior Flags\"] * 3\n",
    "        fraud_score += row[\"Location-Inconsistent Transactions\"] * 2\n",
    "        fraud_score += row[\"Merchant Category Mismatch\"] * 1.5\n",
    "        fraud_score += row[\"User Daily Limit Exceeded\"] * 1.5\n",
    "\n",
    "        # Threshold for fraud detection\n",
    "        return 1 if fraud_score > 4 else 0\n",
    "\n",
    "    # Apply labeling\n",
    "    df[\"Label\"] = df.apply(label_transaction, axis=1)\n",
    "\n",
    "    # Save the dataset to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Dataset saved to {output_file}\")\n",
    "    print(df[\"Label\"].value_counts())  # Print label distribution for verification\n",
    "\n",
    "# Example Usage\n",
    "generate_fraud_detection_dataset(n_samples=10000, output_file=\"fraud_detection_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 3686,
     "status": "error",
     "timestamp": 1734180414132,
     "user": {
      "displayName": "shadabkhan pathan",
      "userId": "14649001743684245130"
     },
     "user_tz": -330
    },
    "id": "vwTGuU-Aww6R",
    "outputId": "71bcfaef-1d5d-4c1b-ff64-a8fd3f9f9855"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_your_file.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-54225423c24f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'path_to_your_file.csv'\u001b[0m  \u001b[0;31m# Replace with your file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfraud_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Initialize scalers and encoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_file.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'path_to_your_file.csv'  # Replace with your file path\n",
    "fraud_data = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize scalers and encoders\n",
    "scaler = MinMaxScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Continuous features to normalize\n",
    "continuous_features = [\n",
    "    'Transaction Amount', 'Behavioral Biometrics', 'Time Since Last Transaction',\n",
    "    'Social Trust Score', 'Account Age', 'Normalized Transaction Amount',\n",
    "    'Transaction Context Anomalies'\n",
    "]\n",
    "\n",
    "# Normalize continuous features\n",
    "fraud_data[continuous_features] = scaler.fit_transform(fraud_data[continuous_features])\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_feature = 'Geo-Location Flags'  # Replace with any other categorical features as needed\n",
    "fraud_data[categorical_feature] = label_encoder.fit_transform(fraud_data[categorical_feature])\n",
    "\n",
    "# Save the preprocessed dataset (optional)\n",
    "output_path = 'preprocessed_fraud_data.csv'\n",
    "fraud_data.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(fraud_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1734181143139,
     "user": {
      "displayName": "shadabkhan pathan",
      "userId": "14649001743684245130"
     },
     "user_tz": -330
    },
    "id": "9egpNz47P4F3",
    "outputId": "1e792df9-1594-4740-a0c0-df197af84927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Transaction Amount  Transaction Frequency Recipient Verification Status  \\\n",
      "0           36.907066                      6                      verified   \n",
      "1           37.879209                      0                      verified   \n",
      "2         1250.985077                      2           recently_registered   \n",
      "3         2972.505568                      3           recently_registered   \n",
      "4          501.334960                      2                      verified   \n",
      "\n",
      "   Recipient Blacklist Status  Device Fingerprinting  VPN or Proxy Usage  \\\n",
      "0                           0                      0                   0   \n",
      "1                           1                      0                   1   \n",
      "2                           1                      0                   0   \n",
      "3                           0                      0                   0   \n",
      "4                           0                      0                   0   \n",
      "\n",
      "  Geo-Location Flags  Behavioral Biometrics  Time Since Last Transaction  \\\n",
      "0             normal              -0.357287                    23.825948   \n",
      "1          high-risk               0.567911                     8.692169   \n",
      "2          high-risk              -0.420052                    14.606622   \n",
      "3          high-risk              -1.795078                    23.791964   \n",
      "4             normal               0.447342                     6.929573   \n",
      "\n",
      "   Social Trust Score  ...  Past Fraudulent Behavior Flags  \\\n",
      "0           17.225762  ...                               0   \n",
      "1           87.512838  ...                               0   \n",
      "2            5.917946  ...                               0   \n",
      "3           72.800664  ...                               1   \n",
      "4           39.941873  ...                               0   \n",
      "\n",
      "   Location-Inconsistent Transactions  Normalized Transaction Amount  \\\n",
      "0                                   0                       0.520577   \n",
      "1                                   0                       0.698691   \n",
      "2                                   1                       0.653676   \n",
      "3                                   1                       0.341925   \n",
      "4                                   0                       0.653939   \n",
      "\n",
      "   Transaction Context Anomalies  Fraud Complaints Count  \\\n",
      "0                      -0.747143                       0   \n",
      "1                       0.631339                       0   \n",
      "2                       0.648835                       0   \n",
      "3                       0.157289                       0   \n",
      "4                      -0.196944                       1   \n",
      "\n",
      "   Merchant Category Mismatch  User Daily Limit Exceeded  \\\n",
      "0                           0                          0   \n",
      "1                           0                          0   \n",
      "2                           0                          0   \n",
      "3                           0                          0   \n",
      "4                           0                          0   \n",
      "\n",
      "   Recent High-Value Transaction Flags  Risk Interaction Score  Label  \n",
      "0                                    0                0.036907      0  \n",
      "1                                    0                5.037879      1  \n",
      "2                                    0                5.250985      1  \n",
      "3                                    0                6.972506      1  \n",
      "4                                    0                0.501335      0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_refined_fraud_dataset(n_samples=10000, fraud_ratio=0.5, seed=42, output_csv=\"refined_fraud_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Generates a refined synthetic fraud detection dataset with balanced fraud cases and realistic features.\n",
    "\n",
    "    Parameters:\n",
    "        n_samples (int): Total number of samples to generate.\n",
    "        fraud_ratio (float): Desired ratio of fraudulent transactions (0 to 1).\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        output_csv (str): Path to save the generated dataset as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The refined fraud detection dataset.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate base features\n",
    "    data = {\n",
    "        \"Transaction Amount\": np.random.exponential(scale=500, size=n_samples),\n",
    "        \"Transaction Frequency\": np.random.poisson(lam=3, size=n_samples),\n",
    "        \"Recipient Verification Status\": np.random.choice(\n",
    "            [\"verified\", \"recently_registered\", \"suspicious\"], n_samples, p=[0.7, 0.2, 0.1]\n",
    "        ),\n",
    "        \"Recipient Blacklist Status\": np.random.choice([0, 1], n_samples, p=[0.95, 0.05]),\n",
    "        \"Device Fingerprinting\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"VPN or Proxy Usage\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"Geo-Location Flags\": np.random.choice(\n",
    "            [\"normal\", \"high-risk\", \"unusual\"], n_samples, p=[0.8, 0.15, 0.05]\n",
    "        ),\n",
    "        \"Behavioral Biometrics\": np.random.normal(loc=0, scale=1, size=n_samples),\n",
    "        \"Time Since Last Transaction\": np.random.uniform(0, 30, n_samples),\n",
    "        \"Social Trust Score\": np.random.uniform(0, 100, n_samples),\n",
    "        \"Account Age\": np.random.uniform(0, 5, n_samples),\n",
    "        \"High-Risk Transaction Times\": np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "        \"Past Fraudulent Behavior Flags\": np.random.choice([0, 1], n_samples, p=[0.95, 0.05]),\n",
    "        \"Location-Inconsistent Transactions\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"Normalized Transaction Amount\": np.random.normal(loc=0.5, scale=0.2, size=n_samples),\n",
    "        \"Transaction Context Anomalies\": np.random.normal(loc=0, scale=1, size=n_samples),\n",
    "        \"Fraud Complaints Count\": np.random.poisson(lam=0.5, size=n_samples),\n",
    "        \"Merchant Category Mismatch\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"User Daily Limit Exceeded\": np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "        \"Recent High-Value Transaction Flags\": np.random.choice([0, 1], n_samples, p=[0.85, 0.15])\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Cap extreme values for realism\n",
    "    caps = {\n",
    "        \"Transaction Amount\": 5000,\n",
    "        \"Behavioral Biometrics\": 3,\n",
    "        \"Time Since Last Transaction\": 30,\n",
    "    }\n",
    "    for feature, cap in caps.items():\n",
    "        df[feature] = df[feature].clip(upper=cap)\n",
    "\n",
    "    # Compute Risk Interaction Score\n",
    "    df[\"Risk Interaction Score\"] = (\n",
    "        df[\"Transaction Amount\"] / 1000 +  # Normalize Transaction Amount\n",
    "        df[\"Recipient Blacklist Status\"] * 2 +\n",
    "        df[\"Past Fraudulent Behavior Flags\"] * 2 +\n",
    "        df[\"VPN or Proxy Usage\"] +\n",
    "        df[\"Geo-Location Flags\"].apply(lambda x: 2 if x == \"high-risk\" else 0)\n",
    "    )\n",
    "\n",
    "    # Assign labels dynamically\n",
    "    fraud_threshold = 5\n",
    "    df[\"Label\"] = (df[\"Risk Interaction Score\"] > fraud_threshold).astype(int)\n",
    "\n",
    "    # Balance fraud cases\n",
    "    fraud_cases = df[df[\"Label\"] == 1]\n",
    "    non_fraud_cases = df[df[\"Label\"] == 0]\n",
    "\n",
    "    # Determine number of samples for each class based on fraud_ratio\n",
    "    n_fraud = int(n_samples * fraud_ratio)\n",
    "    n_non_fraud = n_samples - n_fraud\n",
    "\n",
    "    # Oversample or undersample to achieve balance\n",
    "    fraud_cases = fraud_cases.sample(n=n_fraud, replace=True, random_state=seed)\n",
    "    non_fraud_cases = non_fraud_cases.sample(n=n_non_fraud, replace=True, random_state=seed)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat([fraud_cases, non_fraud_cases]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    balanced_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "    # Example Usage\n",
    "refined_dataset = generate_refined_fraud_dataset(\n",
    "    n_samples=20000,\n",
    "    fraud_ratio=0.5,\n",
    "    seed=42,\n",
    "    output_csv=\"refined_fraud_dataset.csv\"\n",
    ")\n",
    "print(refined_dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1734269166530,
     "user": {
      "displayName": "shadabkhan pathan",
      "userId": "14649001743684245130"
     },
     "user_tz": -330
    },
    "id": "XjilQ_uGZmGN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined JSON saved as mapped_final_transactions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File paths to your JSON files\n",
    "user_friendly_path = \"/content/top_100_transactions.json\"       # Replace with your actual path\n",
    "model_processed_path = \"/content/transactions.json\"\n",
    "\n",
    "# Load user-friendly JSON data\n",
    "with open(user_friendly_path, \"r\") as f:\n",
    "    user_friendly_data = json.load(f)\n",
    "\n",
    "# Load model-processed JSON data\n",
    "with open(model_processed_path, \"r\") as f:\n",
    "    model_processed_data = json.load(f)\n",
    "\n",
    "# Ensure both datasets have the same length\n",
    "min_length = min(len(user_friendly_data), len(model_processed_data))\n",
    "\n",
    "# Combine the two datasets into a list of paired objects\n",
    "combined_data = [\n",
    "    {\"user_friendly\": user_friendly_data[i], \"model_processed\": model_processed_data[i]}\n",
    "    for i in range(min_length)\n",
    "]\n",
    "\n",
    "# Save the combined data to a new JSON file\n",
    "with open(\"mapped_transactions.json\", \"w\") as f:\n",
    "    json.dump(combined_data, f, indent=4)\n",
    "\n",
    "print(\"Combined JSON saved as mapped_final_transactions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1734181386089,
     "user": {
      "displayName": "shadabkhan pathan",
      "userId": "14649001743684245130"
     },
     "user_tz": -330
    },
    "id": "IEu_uaNiQuC_",
    "outputId": "0b6f8548-b8d3-482b-9903-f3e87b592667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Transaction Amount  Transaction Frequency Recipient Verification Status  \\\n",
      "0           36.907066                      6                      verified   \n",
      "1           37.879209                      0                      verified   \n",
      "2         1250.985077                      2           recently_registered   \n",
      "3         2972.505568                      3           recently_registered   \n",
      "4          501.334960                      2                      verified   \n",
      "\n",
      "   Recipient Blacklist Status  Device Fingerprinting  VPN or Proxy Usage  \\\n",
      "0                           0                      0                   0   \n",
      "1                           1                      0                   1   \n",
      "2                           1                      0                   0   \n",
      "3                           0                      0                   0   \n",
      "4                           0                      0                   0   \n",
      "\n",
      "  Geo-Location Flags  Behavioral Biometrics  Time Since Last Transaction  \\\n",
      "0             normal               0.357287                    23.825948   \n",
      "1          high-risk               0.567911                     8.692169   \n",
      "2          high-risk               0.420052                    14.606622   \n",
      "3          high-risk               1.795078                    23.791964   \n",
      "4             normal               0.447342                     6.929573   \n",
      "\n",
      "   Social Trust Score  ...  High-Risk Transaction Times  \\\n",
      "0           17.225762  ...                            0   \n",
      "1           87.512838  ...                            0   \n",
      "2            5.917946  ...                            0   \n",
      "3           72.800664  ...                            0   \n",
      "4           39.941873  ...                            0   \n",
      "\n",
      "   Past Fraudulent Behavior Flags  Location-Inconsistent Transactions  \\\n",
      "0                               0                                   0   \n",
      "1                               0                                   0   \n",
      "2                               0                                   1   \n",
      "3                               1                                   1   \n",
      "4                               0                                   0   \n",
      "\n",
      "   Normalized Transaction Amount  Transaction Context Anomalies  \\\n",
      "0                       0.520577                       0.747143   \n",
      "1                       0.698691                       0.631339   \n",
      "2                       0.653676                       0.648835   \n",
      "3                       0.341925                       0.157289   \n",
      "4                       0.653939                       0.196944   \n",
      "\n",
      "   Fraud Complaints Count  Merchant Category Mismatch  \\\n",
      "0                       0                           0   \n",
      "1                       0                           0   \n",
      "2                       0                           0   \n",
      "3                       0                           0   \n",
      "4                       1                           0   \n",
      "\n",
      "   User Daily Limit Exceeded  Recent High-Value Transaction Flags  Label  \n",
      "0                          0                                    0      0  \n",
      "1                          0                                    0      1  \n",
      "2                          0                                    0      1  \n",
      "3                          0                                    0      1  \n",
      "4                          0                                    0      0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_refined_fraud_dataset(n_samples=10000, fraud_ratio=0.5, seed=42, output_csv=\"refined_fraud_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Generates a refined synthetic fraud detection dataset with balanced fraud cases and realistic features.\n",
    "\n",
    "    Parameters:\n",
    "        n_samples (int): Total number of samples to generate.\n",
    "        fraud_ratio (float): Desired ratio of fraudulent transactions (0 to 1).\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        output_csv (str): Path to save the generated dataset as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The refined fraud detection dataset.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate base features\n",
    "    data = {\n",
    "        \"Transaction Amount\": np.abs(np.random.exponential(scale=500, size=n_samples)),\n",
    "        \"Transaction Frequency\": np.random.poisson(lam=3, size=n_samples),\n",
    "        \"Recipient Verification Status\": np.random.choice(\n",
    "            [\"verified\", \"recently_registered\", \"suspicious\"], n_samples, p=[0.7, 0.2, 0.1]\n",
    "        ),\n",
    "        \"Recipient Blacklist Status\": np.random.choice([0, 1], n_samples, p=[0.95, 0.05]),\n",
    "        \"Device Fingerprinting\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"VPN or Proxy Usage\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"Geo-Location Flags\": np.random.choice(\n",
    "            [\"normal\", \"high-risk\", \"unusual\"], n_samples, p=[0.8, 0.15, 0.05]\n",
    "        ),\n",
    "        \"Behavioral Biometrics\": np.abs(np.random.normal(loc=0, scale=1, size=n_samples)),\n",
    "        \"Time Since Last Transaction\": np.random.uniform(0, 30, n_samples),\n",
    "        \"Social Trust Score\": np.random.uniform(0, 100, n_samples),\n",
    "        \"Account Age\": np.random.uniform(0, 5, n_samples),\n",
    "        \"High-Risk Transaction Times\": np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "        \"Past Fraudulent Behavior Flags\": np.random.choice([0, 1], n_samples, p=[0.95, 0.05]),\n",
    "        \"Location-Inconsistent Transactions\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"Normalized Transaction Amount\": np.abs(np.random.normal(loc=0.5, scale=0.2, size=n_samples)),\n",
    "        \"Transaction Context Anomalies\": np.abs(np.random.normal(loc=0, scale=1, size=n_samples)),\n",
    "        \"Fraud Complaints Count\": np.random.poisson(lam=0.5, size=n_samples),\n",
    "        \"Merchant Category Mismatch\": np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "        \"User Daily Limit Exceeded\": np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "        \"Recent High-Value Transaction Flags\": np.random.choice([0, 1], n_samples, p=[0.85, 0.15])\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Cap extreme values for realism\n",
    "    caps = {\n",
    "        \"Transaction Amount\": 5000,\n",
    "        \"Behavioral Biometrics\": 3,\n",
    "        \"Time Since Last Transaction\": 30,\n",
    "    }\n",
    "    for feature, cap in caps.items():\n",
    "        df[feature] = df[feature].clip(upper=cap)\n",
    "\n",
    "    # Assign labels dynamically\n",
    "    def label_transaction(row):\n",
    "        fraud_score = 0\n",
    "        fraud_score += row[\"Transaction Amount\"] / 1000  # Normalize Transaction Amount impact\n",
    "        fraud_score += row[\"Recipient Blacklist Status\"] * 2\n",
    "        fraud_score += row[\"Past Fraudulent Behavior Flags\"] * 2\n",
    "        fraud_score += row[\"VPN or Proxy Usage\"]\n",
    "        fraud_score += 2 if row[\"Geo-Location Flags\"] == \"high-risk\" else 0\n",
    "        return 1 if fraud_score > 5 else 0\n",
    "\n",
    "    df[\"Label\"] = df.apply(label_transaction, axis=1)\n",
    "\n",
    "    # Balance fraud cases\n",
    "    fraud_cases = df[df[\"Label\"] == 1]\n",
    "    non_fraud_cases = df[df[\"Label\"] == 0]\n",
    "\n",
    "    # Determine number of samples for each class based on fraud_ratio\n",
    "    n_fraud = int(n_samples * fraud_ratio)\n",
    "    n_non_fraud = n_samples - n_fraud\n",
    "\n",
    "    # Oversample or undersample to achieve balance\n",
    "    fraud_cases = fraud_cases.sample(n=n_fraud, replace=True, random_state=seed)\n",
    "    non_fraud_cases = non_fraud_cases.sample(n=n_non_fraud, replace=True, random_state=seed)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat([fraud_cases, non_fraud_cases]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    balanced_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "    # Example Usage\n",
    "refined_dataset = generate_refined_fraud_dataset(\n",
    "    n_samples=20000,\n",
    "    fraud_ratio=0.5,\n",
    "    seed=42,\n",
    "    output_csv=\"fraud_dataset.csv\"\n",
    ")\n",
    "print(refined_dataset.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 9894,
     "status": "error",
     "timestamp": 1734258941598,
     "user": {
      "displayName": "shadabkhan pathan",
      "userId": "14649001743684245130"
     },
     "user_tz": -330
    },
    "id": "GxxPeY564wMG",
    "outputId": "851c6a7d-9dc3-47ae-e320-380d8f457fb8"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "15 columns passed, passed data had 20 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_or_indexify_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;31m# caller's responsibility to check for this...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             raise AssertionError(\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0;34mf\"{len(columns)} columns passed, passed data had \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 15 columns passed, passed data had 20 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-72a445684ed3>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Map the input data to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mraw_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumerical_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Initialize the scaler (you must fit it using the training data MinMaxScaler)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                     arrays, columns, index = nested_data_to_arrays(\n\u001b[0m\u001b[1;32m    852\u001b[0m                         \u001b[0;31m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                         \u001b[0;31m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m     \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_finalize_columns_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;31m# GH#26429 do not raise user-facing AssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 15 columns passed, passed data had 20 columns"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Input JSON data\n",
    "raw_data = {\n",
    "    \"features\": [\n",
    "        36.90706552196081, 6, \"verified\", 0, 0, 0, \"normal\",\n",
    "        0.3572874504464022, 23.82594842084447, 17.22576154101364, 3.934289851050419,\n",
    "        0, 0, 0, 0.5205771170664302, 0.747143472779124, 0, 0, 0, 0\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define column names (match original training data order)\n",
    "numerical_cols = [\n",
    "    \"Transaction Amount\", \"Transaction Frequency\", \"Behavioral Biometrics\",\n",
    "    \"Time Since Last Transaction\", \"Social Trust Score\", \"Account Age\",\n",
    "    \"High-Risk Transaction Times\", \"Normalized Transaction Amount\",\n",
    "    \"Transaction Context Anomalies\", \"Fraud Complaints Count\",\n",
    "    \"Merchant Category Mismatch\", \"User Daily Limit Exceeded\",\n",
    "    \"Recent High-Value Transaction Flags\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\"Recipient Verification Status\", \"Geo-Location Flags\"]\n",
    "\n",
    "# Map the input data to a DataFrame\n",
    "raw_features = pd.DataFrame([raw_data[\"features\"]], columns=numerical_cols + categorical_cols)\n",
    "\n",
    "# Initialize the scaler (you must fit it using the training data MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Example of fitting the scaler (use your original training data for fitting)\n",
    "# scaler.fit(X_train[numerical_cols])\n",
    "\n",
    "# Scale numerical features\n",
    "raw_features[numerical_cols] = scaler.transform(raw_features[numerical_cols])\n",
    "\n",
    "# One-hot encode categorical features\n",
    "raw_features = pd.get_dummies(raw_features, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ensure the feature order matches the training data\n",
    "# This step requires you to know the final column order after training preprocessing\n",
    "final_order = [...]  # Replace with the correct column order from your training set\n",
    "raw_features = raw_features.reindex(columns=final_order, fill_value=0)\n",
    "\n",
    "# Convert to a list for model input\n",
    "preprocessed_features = raw_features.iloc[0].tolist()\n",
    "\n",
    "print(preprocessed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmZx6OoCww_a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOE+KfNL76lrFpwUNsT5Ize",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
